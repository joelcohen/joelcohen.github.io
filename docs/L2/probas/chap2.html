---
layout: default
permalink: /docs/L2/probas/chap2
title: Probabilités - Chapitre 2
hidden : True
---
<script type="text/javascript">
<!--
    function toggle_visibility(id) {
       var elt = document.getElementById(id);
       if(elt.style.display == 'block')
          elt.style.display = 'none';
       else
          elt.style.display = 'block';
    }
//-->
</script>
<div class="exomath">
    <div class="titlebox">
        <h2>Chapitre 2</h2>
        <h1>Variables aléatoires</h1>
    </div>
    <section>
        <h2>Variable aléatoire</h2>
        
        <h3>Définition</h3>
        <p>Jusqu'ici nous avons parlé d'ensembles (univers, événements). Il est donc naturel de construire des fonctions entre espaces probabilisables, c'est la notion de variable aléatoire. Cela est notamment utile quand on calcule une quantité qui dépend du résultat d'une expérience aléatoire (vos gains à un jeu de pile de face, somme des valeurs deux dés, etc).</p>
        
        <article class="definition" id="def_variable"><strong>Variable aléatoire</strong>
            <p>Soient $(\Omega, \mathcal{A})$ et $(E, \mathcal{E})$ des espaces probabilisables. Une <strong class="new">variable aléatoire</strong> sur $\Omega$ à valeurs dans $E$ est une fonction $X : \Omega \longmapsto E$ telle que pour tout événement $B \in \mathcal{E}$, on a</p>
            <p>$$X^{-1}(B) = \set{\omega \in \Omega, \ X(\omega) \in B} \in \mathcal{A}$$</p>
            Pour alléger les notations, l'événement $X^{-1}(B)$ sera noté $\set{X \in B}$. Dans le cas où $B = \set{b}$, on notera même $X^{-1}(\{b\}) = \set{X = b}$.
        </article>
        
        <p>Moralement, on peut penser à $X$ comme une variable (à valeur dans l'ensemble $E$) qui est tirée au hasard. C'est ce qui explique le nom de variable aléatoire.</p>
        <article class="culturel">
            <p>La condition que $X^{-1}(B) \in \mathcal{A}$ pour tout $B \in \mathcal{E}$ signifie que la fonction $X$ est <strong class="new">mesurable</strong>. Dans la plupart des exemples que l'on considère dans ce cours, l'ensemble $\Omega$ est fini, et la tribu $\mathcal{A}$ est l'ensemble $\mathcal{P}(\Omega)$ de toutes les parties de $\Omega$. Dans ce cas, on peut oublier la condition de mesurabilité, qui sera automatiquement vérifiée. Et dans le cas général, il est assez difficile de fabriquer une fonction $X$ qui ne serait pas mesurable (on doit au mathématicien italien Guiseppe la construction en 1905 du premier exemple d'un <a href="https://fr.wikipedia.org/wiki/Ensemble_de_Vitali" target="new">ensemble non mesurable</a>).</p>
        </article>
        
        
        <article class="exemple"><strong>Pile je gagne, face tu perds</strong>
            <p>On joue à pile ou face, le joueur lance une pièce de monnnaie, si elle tombe sur pile, il gagne 1€, sinon il perd 1€. L'univers associé à cette expérience aléatoire est $\Omega = \set{\pile, \face}$. On peut définit sur $\Omega$ la variable aléatoire $X : \Omega \to \set{-1,1}$ qui représente les gains du joueur, en posant</p>
            $$X(\pile) = 1, \qquad X(\face) = -1$$
        </article>
        <article class="exemple"><strong>Somme de deux dés</strong>
            <p>On lance deux dés, on note $S$ la variable aléatoire qui représente la somme des valeurs des dés.</p>
            
            <p>Formellement, l'univers de notre expérience est $\Omega = \ints{1,6}^2$ Cela veut dire qu'on considère la fonction $S : \Omega \to \N$ définie pour $\omega = (x,y) \in \Omega$ par</p>
            $$S(\omega) = x + y$$
        </article>
        <article class="exemple"><strong>Le Scrabble</strong>
            <p>On joue au scrabble, on tire une lettre au sort dans le sachet. On note $V$ la variable aléatoire qui représente la valeur en point de la lettre (en français).</p>
            <p>Formellement, on a donc $\Omega = \set{A, B, C, \ldots, Y, Z, \text{joker}}$, et $V(A) = 1$, $V(B) = 3$, $V(C) = 3$, $V(D) = 2$, …, $V(Z) = 10$ et $V(\text{joker}) = 0$ (voir <a href="https://www.regles-de-jeux.com/regle-du-scrabble/" target="new">ici</a> pour la liste complète).</p>
        </article>
        <article class="exemple"><strong>Temps d'arrêt</strong>
            <p>On lance une pièce jusqu'à ce qu'on fasse pile. On note $N$ la variable aléatoire qui représente le nombre de lancers qu'on a fait (et on pose $N = +\infty$ dans l'éventualité où on ne tombe jamais sur pile).</p>
        </article>
        
        <h3>Variable discrète, réelle</h3>
        <p>Si l'ensemble $E$ des valeurs d'une variable aléatoire est fini ou dénombrable, on dira que la variable est <strong class="new">discrète</strong>. Si $E = \R$ on parlera d'une <strong class="new">variable aléatoire réelle</strong>. Il est même possible de prendre un espace vectoriel (comme par exemple $E = \R^n$) pour ensemble des valeurs de la variable, on parle alors de <strong class="new">vecteur aléatoire</strong>. Pour des raisons de simplicité, on étudiera principalement les variables aléatoires discrètes dans ce cours.</p>
    </section>
    <section>
        <h2>Loi d'une variable aléatoire</h2>
        
        <article class="prop" id="loi_variable"><strong>Loi d'une variable aléatoire</strong>
            <p>Soit $(\Omega, \mathcal{A}, \P)$ un espace probabilisé, $(E, \mathcal{E})$ un espace probabilisable, et $X : \Omega \to E$ une variable aléatoire sur $\Omega$ à valeurs dans $E$. Alors la fonction</p>
            $$\begin{align*}
            \P_X : \mathcal{E} & \longmapsto [0,1] \\
            \qquad B & \longmapsto \P(X \in B)
            \end{align*}$$
            <p>est une probabilité sur l'espace probabilisable $(E, \mathcal{E})$, appelée <strong class="new">loi de la variable</strong> $X$.</p>
        </article>
        <p>On fera attention au fait que la loi d'une variable aléatoire est une probabilité sur l'ensemble $E$ des valeurs et pas sur l'espace de départ $\Omega$. C'est d'ailleurs un des grands bénéfices pratiques de la notion de variable aléatoire, elle permet parfois de laisser un flou artistique sur l'ensemble $\Omega$ (potentiellement complexe) et se concentrer sur l'ensemble des valeurs.</p>
        <p>Bien souvent, on commet l'abus de confondre une variable aléatoire avec sa loi (en toute impunité avouons-le) mais il faut bien comprendre que des variables aléatoires distinctes peuvent avoir la même loi. Par exemple, sur $\Omega = \set{\pile, \face}$ on peut définir les variable aléatoires $X, Y : \Omega \to \set{-1,1}$ par</p>
        <p>$$X(\pile) = Y(\face) = 1, \qquad X(\face) = Y(\pile) = -1$$</p>
        <p>Les variables $X$ et $Y$ sont distinctes, et pourtant</p>
        <p><span class="inlineblock">$$\P(X = 1) = \P(Y = 1) = \frac{1}{2}$$</span>
                <span class="inlineblock">$$\P(X = -1) = \P(Y = -1) = \frac{1}{2}$$</span></p>
        <p>donc $X$ et $Y$ ont la même loi.</p>
        
        <h3>Loi d'une variable aléatoire discrète</h3>
        <p>Dans le cas d'une variable aléatoire discrète, la loi de $X$ est entièrement déterminée par les valeurs $\P(X = e)$ pour $e \in E$. En effet, si $B \in \mathcal{E}$, alors on peut calculer $\P(X \in B)$ avec la formule :</p>
        <p>$$\P(X \in B) = \sum_{b \in B} \P(X = b)$$</p>
        <p>En particuliers les valeurs $\P(X = e)$ doivent se sommer à $1$ au total :</p>
        <p>$$\sum_{e \in E} \P(X = e) = \P(\Omega) = 1$$</p>
        <h3>Variable discrète et probabilités totales</h3>
        <p>Remarquons que si $X : \Omega \to E$ est une variable aléatoire sur l'espace probabilisé $(\Omega, \mathcal{A}, \P)$, alors la famille d'événements $(\set{X = e})_{e \in E}$ forme une <a href="chap1#partition">partition</a> de $\Omega$. Alors d'après la <a href="chap1#probas_totales">formule des probabilités totales</a>, pour tout événement $A \in \mathcal{A}$, on a</p>
        <p>$$\P(A) = \sum_{e \in E, \ \P(X = e) \ne 0} \P(A | X = e) \cdot \P(X = e)$$</p>
        <p>Par exemple, on peut réinterpréter le calcul fait dans <a href="td1#proba_conditionelle">cet exercice</a> en termes de variables aléatoires. On lance un dé dix fois de suite. Si on note $N$ l'indice du premier lancer auquel on obtient un $6$, en prenant $N = 11$ si on ne fait jamais 6 (notons d'ailleurs que si la variable aléatoire $X_i$ désigne le résultat du $i^{\text{ème}}$ lancer de dé, alors $N = \min \set{i \in \ints{1,10}, X_i = 6}$). Alors les événements $(\set{N = n})_{1 \le n \le 11}$ forment une partition de l'univers $\Omega$. Si $B$ désigne l'événement "faire au moins un 6", d'après la formule des probabilités totales, on a</p>
        <p>$$\P(B) = \sum_{n = 1}^{11} \P(B | N = n) \cdot \P(N = n) = \sum_{n = 1}^{10} \pars{\frac{5}{6}}^{n-1} \cdot \frac{1}{6} = 1 - \pars{\frac{5}{6}}^{10}$$</p>
        <article class="exemple"><strong>Les déplacements au Monopoly</strong>
            <p>On joue (sans tricher) au Monopoly. On lance deux dés, on note $S$ la variable aléatoire qui représente la somme des valeurs des dés. Au chapitre précédent, on a calculé $\P(S = 7) = \frac{1}{6}$ et $\P(S = 5) = \frac{1}{9}$. Plus généralement, on peut calculer que pour $2 \le k \le 12$, on a</p>
            <p>$$\P(S = k) = \frac{6 - |k-7|}{36}$$</p>
            <p>On peut mettre résumer dans un tableau</p>
            <table class="dist">
                <tr>
                    <th>$k$</th>
                    <td>2</td>
                    <td>3</td>
                    <td>4</td>
                    <td>5</td>
                    <td>6</td>
                    <td>7</td>
                    <td>8</td>
                    <td>9</td>
                    <td>10</td>
                    <td>11</td>
                    <td>12</td>
                </tr>
                <tr>
                    <th>$\P(S = k)$</th>
                    <td>$\frac{1}{36}$</td>
                    <td>$\frac{2}{36}$</td>
                    <td>$\frac{3}{36}$</td>
                    <td>$\frac{4}{36}$</td>
                    <td>$\frac{5}{36}$</td>
                    <td>$\frac{6}{36}$</td>
                    <td>$\frac{5}{36}$</td>
                    <td>$\frac{4}{36}$</td>
                    <td>$\frac{3}{36}$</td>
                    <td>$\frac{2}{36}$</td>
                    <td>$\frac{1}{36}$</td>
                </tr>
            </table>
            <p>On peut visualiser le résultat par un histogramme</p>
            <figure class="fig">
                <img src="somme_des.png" alt="Histogramme représentant la loi de la somme de deux dés">
                <div class="caption">Histogramme représentant la loi de la somme de deux dés</caption>
            </figure>
            
        </article>
        <article class="exemple"><strong>Score au Scrabble</strong>
            <p>On reprend l'exemple du jeu de Scrabble et du tirage d'une lettre dans le sachet. On considère la variable aléatoire $V$ qui désigne sa valeur en points. On a $\set{V = 0} = \set{\text{joker}}$, donc $\P(V = 0) = \frac{1}{27}$. On a $\set{V = 1} = \set{A, E, I, L, N, O, R, S, T, U}$, donc $\P(V = 1) = \frac{10}{27}$ … $\P(V = 10) = \frac{5}{27}$</p>
            <table class="dist">
                <tr>
                    <th>$k$</th>
                    <td>0</td>
                    <td>1</td>
                    <td>2</td>
                    <td>3</td>
                    <td>4</td>
                    <td>5</td>
                    <td>6</td>
                    <td>7</td>
                    <td>8</td>
                    <td>9</td>
                    <td>10</td>
                </tr>
                <tr>
                    <th>$P(V = k)$</th>
                    <td>$\frac{1}{27}$</td>
                    <td>$\frac{10}{27}$</td>
                    <td>$\frac{3}{27}$</td>
                    <td>$\frac{3}{27}$</td>
                    <td>$\frac{3}{27}$</td>
                    <td>$0$</td>
                    <td>$0$</td>
                    <td>$0$</td>
                    <td>$\frac{2}{27}$</td>
                    <td>$0$</td>
                    <td>$\frac{5}{27}$</td>
                </tr>
            </table>
            <p>Ce qui ne semble suivre aucune règle claire, il y a fort à parier que les concepteurs du jeu ont choisi ces scores de manière un peu arbitraire.</p>
        </article>
        <article class="exemple"><strong>Temps nécessaire pour faire un 6</strong>
        <p>On lance un dé. Tant qu'on ne fait pas 6, on recommence. On considère la variable aléatoire $N$ qui désigne le nombre de lancers qu'on fait (et on considère que $N = +\infty$ si on ne fait jamais $6$). On calcule alors pour tout $k \in \N$,</p>
        $$\P(N = k) = \pars{\frac{5}{6}}^{k-1} \cdot \frac{1}{6}$$
        <p>ce qui décroit géométriquement vers 0 (il est de de plus improbable de faire des longues séquences sans 6), et on a $\P(N = +\infty) = 0$.</p>
        </article>
        <p>Pour d'autres exemples de calculs de lois, voir <a href="td2#min_max">la feuille d'exercies 2</a>.</p>
        
        
        <h3>Loi d'une variable réelle</h3>
        <p>Dans le cas d'une variable aléatoire réelle $X$, la loi de $X$ peut être caractérisée par la fonction $F_X : x \mapsto \P(X \le x)$, qu'on appelle la <strong class="new">fonction de répartition</strong> de $X$.</p>
     <article class="definition"><strong>Fonction de répartition, densité</strong>
            <p>Soit $(\Omega, \mathcal{A}, \P)$ un espace probabilisé et $X : \Omega \to \R$ une variable aléatoire réelle. On appelle <strong class="new">fonction de répartition</strong> de $X$, la fonction réelle $F_X : \R \to [0,1]$ définie pour $x \in \R$ par</p>
            $$F_X(x) = \P(X \le x)$$
            <p>Si la fonction $F_X$ est $\mathcal{C}^1$, on dit que la variable $X$ admet une densité. On appelle alors <strong>densité de probabilité</strong> la fonction $p_x : \R \to \R$ dérivée de $F_X$ :</p>
            $$p_X = F_X'$$
            
        </article>
        
        <p>Si $X$ admet une densité $p_X$, alors on a</p>
        <p>$$\P(a \le X \le b) = \int_a^b p_X(u) \, du$$</p>
    </section>
    <section>
        <h2>Indépendance de variables aléatoires</h2>
        <p>On dira que deux variables aléatoires $X$ et $Y$ sont indépendantes si les événements $\set{X \in B}$ $\set{Y \in C}$ sont <a href="chap1#independant">indépdendants</a> pour tous événements $B$ et $C$, ce qui revient à demander que la loi jointe de la variable $(X,Y)$ soit le produit des lois de $X$ et $Y$.</p>
        <article class="definition" id="independance_variables"><strong>Indépendance de variables aléatoires</strong>
            <p>Soit $(\Omega, \mathcal{A}, \P)$ un espace probabilisé et $(X_i)_{i \in I}$ une famille de variables aléatoires toute définies sur $\Omega$. On dira que les $X_i$ sont <strong class="new">indépendantes</strong> si pour toute partie finie $J \subset I$ on a</p>
            <p>$$\P \pars{\forall j \in J, X_j \in A_j} = \prod_{j \in J} \P(A_j)$$</p>
            <p>En particulier, deux variables aléatoires discrètes $X : \Omega \to E_1$ et $Y : \Omega \to E_2$ sont indépendantes si pour tout $(x,y) \in E_1 \times E_2$, on a</p>
            <p>$$\P(X = x, Y = y) = \P(X = x) \times \P(Y = y)$$</p>
        </article>
        <p>Remarquons que l'indépendance des variables aléatoire revient à demander l'indépendance mutuelle des événements $(\set{X_i = x_i})_{i \in I}$ pour toutes valeurs $(x_i) \in \prod_{i \in I} E_i$ (et pas seulement l'indépendance 2 à 2).</p>
        <p>L'indépendance des variables aléatoires correspond à la notion intuitive que la valeur de l'une des variables ne dépend pas des autres.</p>
        <article class="exemple" id="lancers_successifs"><strong>Lancers successifs de dés</strong>
            <p>On lance un dés deux fois de suite. Soit $X_1$ la valeur obtenue au premier lancer, et $X_2$ la valeur obtenue second lancer. On comprend intuitivement que $X_1$ et $X_2$ sont indépendants : la valeur du premier lancer n'a a priori aucune influence sur celle du second lancer. Mais pour se familiariser avec la définition, donnons la preuve formelle. Alors pour $i \in \ints{1,6}$ fixé, on a</p>
            <p>$$\set{X_1 = i} = \set{(i, y), y \in \ints{1,6}}$$</p>
            <p>et de même, pour $j \in \ints{1,6}$ fixé, on a</p>
            <p>$$\set{X_1 = j} = \set{(x, j), x \in \ints{1,6}}$$</p>
            <p>Donc pour tous $i, j \in \ints{1,6}$, on a</p>
            <p>$$\P(X_1 = i) = \P(X_2 = j) = \frac{6}{36} = \frac{1}{6}$$</p>
            <p>et par ailleurs</p>
            <p>$$
                \begin{align*}
                    \P(X_1 = i, X_2 = j) &= \P(\set{(i,j)})\\
                    &= \frac{1}{36}\\
                    &= \P(X_1 = i) \times \P(X_2 = j)
                \end{align*}
                $$</p>
                donc les variables $X_1$ et $X_2$ sont indépendantes. Plus généralement, si on lance un dé successivement $n$ fois et qu'on note $X_i$ le résultat du $i^{\text{ème}}$ lancer, alors les variables $(X_{i})_{1 \le i \le n}$ sont idépendantes.
        </article>
    </section>
    <section>
        <h2>Loi usuelles</h2>
        <p>Dans cette partie, on établit une liste une série non exhaustive de loi de variables aléatoires discrètes que l'on rencontre fréquemment.</p>
        

        
        <h3>Loi uniforme</h3>
        <p>Pour $X$ une variable aléatoire à valeurs dans un ensemble fini $E$, on dit que la variable aléatoire $X$ suit une <strong class="new">loi uniforme sur $E$</strong> si toutes les valeurs sont équiprobables. On a alors pour tout $x \in E$ la probabilité $\P(X = x) = \frac{1}{|E|}$.</p>
        <p>En particulier, si $X$ suit loi uniforme sur $\ints{a,b}$ on a $\P(X = k) = \frac{1}{b-a+1}$ pour tout $a \le k \le b$.</p>
        <article class="exemple">
            On lance un dé cubique à 6 face. On note $D$ la variable aléatoire qui représente le résultat du lancer. Alors la variable $D$ suit une loi uniforme sur $\ints{1,6}$.
        </article>
        
        
        <h3>Loi de Bernouilli</h3>
        <p>On dit que la variable aléatoire $X$ suit une <strong class="new">loi de Bernouilli</strong> si la variable est à valeurs dans $\set{0,1}$. La valeur $p = \P(X = 1)$ est appelée le <strong class="new">paramètre</strong> de la loi (ou on dit encore que $X$ suit une loi de Bernouilli de paramètre $p$). On a $\P(X = 0) = 1-p$.</p>
        <p>Si $X$ est une variable aléatoire à valeurs dans $\set{-1,1}$, on dira qu'elle suit une <strong class="new">loi de Rademacher</strong> de paramètre $p = \P(X = 1)$. On a alors $\P(X = -1) = 1 -p$.</p>   
        <p>Les deux loi sont réliées. En effet on remarque que $X$ suit une loi de Bernouilli de paramètre $p$ si et seulement si $2X-1$ suit une loi de Rademacher de paramètre $p$.</p>
        <article class="exemple">On tire une carte au hasard dans un paquet de 52 cartes, on note $X$ la variable aléatoire qui vaut $1$ si la carte tirée est un roi, et $0$ sinon. Alors $X$ suit une loi de Bernouilli de paramètre $\P(X=1) = \P(\text{"tirer un roi"})= \frac{4}{52} = \frac{1}{13}$.</article>
        <article class="exemple">Une urne contient 99 boules noires, et une boule blanche. On tire une boule au hasard dans l'urne et on note $B$ la variable aléatoire qui vaut $1$ si la boule est blanche, et $0$ sinon. Alors la variable $B$ suit une loi de Bernouilli de paramètre $\frac{1}{100}$.</article>
        <article class="exemple">On joue à deux à un jeu de pile ou face avec pièce de monnaie équilibrée. Chacun mise 1€, et si la pièce tombe sur pile, je gagne 1€, sinon je perds 1€. Alors la variable $G$ qui représente mon gain (négatif si je perds) suit une loi de Rademacher de paramètre $\frac{1}{2}$.</article>
        
        <h3>Loi binomiale</h3>
        <p>C'est la loi que suit une somme de variables de Bernouilli indépendantes. Précisément, si les $(X_i)_{1 \le i \le n}$ sont $n$ variables de Bernouilli indépendantes de même paramètre $p \in [0,1]$, on dira que la variable</p>
        $$Y = \sum_{i = 1}^n X_i$$
        <p>suit une <strong class="new">loi binomiale</strong> de paramètres $(n,p)$. La variable $Y$ prend ses valeurs dans $\ints{0,n}$, et pour tout $k \in \ints{0,n}$, on a</p>
        $$\P(Y = k) = \binom{n}{k} p^k (1-p)^{n-k}$$
        <p>La loi binomiale apparaît comme le nombre de succès lorsqu’on répète une même expérience aléatoire dont la probabilité de succès est $p$.</p>
        <article class="exemple">
            On lance une pièce équilibrée $10$ fois et on note $N$ la variable aléatoire qui représente le nombre de fois que l'on est tombé sur face. Alors $F$ suit une loi binomiale de paramètres $(10, 1/2)$ : en effet, si on note $X_i$ la variable aléatoire qui vaut $1$ si on fait face au $i^{\text{ème}}$ lancer et $0$ sinon, alors les variables $(X_i)_{1 \le i \le 20}$ suivent des lois de Bernouilli de paramètre $\frac{1}{2}$, sont indépendantes, et on a
            $$F = \sum_{i = 1}^{20} X_i$$
            Si $k \le n$, la probabilité d'avoir obtenu $k$ fois face est donnée par
            $$\P(F = k) = \binom{20}{k} p^k (1-p)^{20-k}$$
            Car il faut faire $k$ lancers avec face, $20-k$ lancer avec pile, et il y a $\binom{20}{k}$ façon de choisir l'ordre des piles et faces obtenus.
        </article>
        
        
        <h3>Loi géométrique</h3>
        <p>On dit que la variable $X$ suit une <strong class="new">loi géométrique</strong> de paramètre $p \in [0,1]$ si $X$ est à valeurs dans $\N^*$ et si pour tout $n \in \N^*$, on a</p>
        <p>$$\P(X = n) = p (1-p)^{n-1}$$</p>
        <p>La loi géométrique apparaît comme le temps nécessaire pour obtenir un premier succès lorsqu’on répète une même expérience aléatoire dont la probabilité de succès est $p$. Formellement, si $(X_n)_{n \in \N}$ est une suite de variables aléatoires de Bernouilli de paramètre $p$ et indépendantes, alors la variable aléatoire</p>
        <p>$$N = \min \set{n \in \N, X_n = 1}$$</p>
        <p>suit une loi géométrique de paramètre $p$. On remarque que la somme des probabilités $\P(X = n)$ pour $n$ variant dans $\N$ fait bien à $1$ au total :</p>
        <p>$$
            \begin{align*}
            \sum_{n = 1}^{+\infty} \P(X = n) &= p \sum_{n = 1}^{+\infty} (1-p)^{n-1}\\
             &= p \sum_{k = 0}^{+\infty} (1-p)^{k} \\
             &= p \cdot \frac{1}{1- (1-p)} \\
             &= 1
            \end{align*}$$</p>
        <article class="exemple">
            On lance le dé de manière répétée jusqu'à ce qu'on fasse un $6$. Si on fait $6$ on s'arrête de lancer le dé, sinon on continue. On note $N$ la variable aléatoire qui représente le nombre de lancer qu'on a fait au total. On vérifie que pour $n \in \N$, on a
            $$\P(N = n) = \pars{\frac{5}{6}}^{n-1} \frac{1}{6}$$
            car si on doit faire $n$ lancer pour atteindre 6, c'est qu'on a fait un 6 sur le dernier lancer (probabilité $1/6$) et autre chose qu'un 6 sur les $n-1$ premiers lancers (probabilité $5/6$ pour chacun). Donc la variable $N$ suit une loi géométrique de paramètre $\frac{1}{6}$.
        </article>
        
        
        <h3>Loi de Poisson</h3>
        <p>On dit qu'une variable aléatoire suit une loi de Poisson de paramètre $\lambda > 0$, si $X$ est à valeurs dans $\N$ et pour tout $k \in \N$ on a</p>
        <p>$$\P(X = k) = e^{-\lambda} \frac{\lambda^k}{k!}$$</p>
        <p>On vérifie que la somme des probabilités $\P(X = k)$ vaut bien $1$ au total :</p>
        <p>$$
            \begin{align*}
            \sum_{k= 0}^{+\infty} \P(X = k) &= e^{-\lambda} \sum_{k = 0}^{+\infty} \frac{\lambda^k}{k!}\\
             &= e^{-\lambda} e^{\lambda} \\
             &= 1
            \end{align*}$$
        </p>
        <p>Une utilité de la loi de Poisson est qu'elle permet d'approximer la loi binomiale. Précisément, si $n$ est très grand et $p$ petit, la loi binomiale de paramètre $(n, p)$ est proche de la loi de Poisson de paramètre $\lambda = np$. En conséquence, la loi de Poisson permet notamment de modéliser le nombre d'occurence d'un phénomènes rare ($p$ petit) dans une grande population ($n$ grand).</p>
        <p>Initialement cantonnée à modéliser des événements rares comme les arrivées de bateaux dans un port ou les accidents dus aux coups de pied de cheval dans les armées (source <a href="https://fr.wikipedia.org/wiki/Loi_de_Poisson#Domaines_d'application" target="new">Wikipédia</a>). Son champs d'applications s'est considérablement élargi depuis, et on l'utilise désormais pour modéliser de nombreux phénomènes comme la charge d'un réseau de télécommunication (nombre de personnes connectées à un moment donnée), la durée de vie de composant électroniques, le temps de vie d'une particule radioactive, le taux de mutations aléatoires, la demande de passagers pour un avion, la probabilité de défaut d'un crédit (probabilité qu'une personne ne puisse pas rembourser son crédit), etc.</p>
    </section>
    <nav>
        <a href="chap1"><< Chapitre précédent : Événements</a>
    </nav>
</div>   